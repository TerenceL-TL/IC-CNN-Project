{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-based Brain Tumour Segmentation Network\n",
    "## Import packages\n",
    "Please make sure you have all the required packages installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 22:10:15.114434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:10:15.114718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:10:15.114830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:10:15.114973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:10:15.115060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-10 22:10:15.115126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3112 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/home/terence/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/client/session.py:1769: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import cv2\n",
    "import keras\n",
    "import ipywidgets as widgets\n",
    "import keras_tuner as kt\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.widgets import Slider\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import image as tf_image\n",
    "from tensorflow import io as tf_io\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise MRI Volume Slices and Segmentation Maps\n",
    "Each MRI image contains information about a three-dimensional (3D) volume of space. An MRI image is composed of a number of voxels, which is like pixels in 2D images. Here try to visualise the axial plane (usually has a higher resolution) of some of the volumes and the corresponding segmentation maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a4dd5bce4142a48ad846be88079fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Data Select', options=('Nothing Selected', '001', '002', '003', '004', '005', '006', '00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8506bc2e77ae40f2926345e3c34a433f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='val', max=154), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Visualization\n",
    "# Choose set on the selection bar, then use the trackbar for moving up and down\n",
    "\n",
    "# messages\n",
    "no_selection_hint = \"Nothing Selected\"\n",
    "\n",
    "# path related\n",
    "img_path = 'dataset_segmentation/'\n",
    "train_path = os.path.join(img_path, \"train\")\n",
    "\n",
    "# global variables \n",
    "view_pla_path = None\n",
    "view_seg_path = None\n",
    "view_pla_load = None\n",
    "view_seg_load = None\n",
    "slices = None\n",
    "sliders = None\n",
    "\n",
    "def update_slice(val):\n",
    "    global view_pla_load\n",
    "    global view_seg_load\n",
    "    imgfla = view_pla_load[:,:,val]\n",
    "    imgseg = view_seg_load[:,:,val]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(imgfla, cmap='gray')\n",
    "    plt.title('FLA')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(imgseg, cmap='gray')\n",
    "    plt.title('SEG')\n",
    "    plt.show()\n",
    "    return()\n",
    "\n",
    "def update_set(strval):\n",
    "    global view_pla_path \n",
    "    global view_seg_path\n",
    "    global view_pla_load\n",
    "    global view_seg_load\n",
    "    global slices\n",
    "    global sliders\n",
    "    try:\n",
    "        if sliders is not None:\n",
    "            sliders.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "    if strval['type'] == 'change' and strval['name'] == 'value':\n",
    "        set_str = strval['new']\n",
    "    # print(set_str)\n",
    "    if set_str == no_selection_hint:\n",
    "        return()\n",
    "    view_pla_path = os.path.join(train_path, set_str, set_str + \"_fla.nii.gz\")\n",
    "    view_seg_path = os.path.join(train_path, set_str, set_str + \"_seg.nii.gz\")\n",
    "\n",
    "    view_pla_load = nib.load(view_pla_path).get_fdata()\n",
    "    view_seg_load = nib.load(view_seg_path).get_fdata()\n",
    "\n",
    "    slices = view_pla_load.shape\n",
    "    sliders = interactive(update_slice, val=widgets.IntSlider(value=0, min=0, max=slices[2]-1, step=1) )\n",
    "    display(sliders)\n",
    "    return()\n",
    "\n",
    "dataset_subfolder = []\n",
    "\n",
    "for CLASS in os.listdir(train_path):\n",
    "    if not CLASS.startswith('.'):\n",
    "        dataset_subfolder.append(CLASS)\n",
    "\n",
    "dataset_subfolder.sort()\n",
    "dataset_subfolder.insert(0, no_selection_hint)\n",
    "\n",
    "dropdown = widgets.Dropdown(options=dataset_subfolder, value=no_selection_hint, description='Data Select')\n",
    "\n",
    "dropdown.observe(update_set, names='value')\n",
    "display(dropdown)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Segmentation\n"
     ]
    }
   ],
   "source": [
    "# Segmentation of DataSets for training and test\n",
    "\n",
    "#uncomment if using linux/macos\n",
    "!rm -rf Train Val\n",
    "!mkdir Train Val \n",
    "!mkdir Train/Image Train/Target Val/Image Val/Target\n",
    "\n",
    "#uncomment if using windows\n",
    "# !rmdir Train Val /s /q\n",
    "# !md Train Val Train\\Yes Train\\No Val\\Yes Val\\No\n",
    "\n",
    "def nii_to_jpg(nii_file, npy_file):\n",
    "    nii_img = nib.load(nii_file)\n",
    "    \n",
    "    nii_data = nii_img.get_fdata()\n",
    "\n",
    "    num_slices = nii_data.shape[2]\n",
    "    \n",
    "    # Save each slice as a separate .npy file\n",
    "    for i in range(num_slices):\n",
    "        slice_data = nii_data[:, :, i]\n",
    "        jpg_file_slice = npy_file + f'_slice_{i}.jpg'\n",
    "        img = Image.fromarray(slice_data, mode='L')\n",
    "        img.save(jpg_file_slice)\n",
    "    return()\n",
    "\n",
    "# data segmentation lists\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "dir_list = os.listdir(train_path)\n",
    "random.shuffle(dir_list)\n",
    "data_num = len(dir_list)\n",
    "for (n, file_name) in enumerate(dir_list):\n",
    "    if not file_name.startswith('.'):\n",
    "        view_pla_path = os.path.join(train_path, file_name, file_name + \"_fla.nii.gz\")\n",
    "        view_seg_path = os.path.join(train_path, file_name, file_name + \"_seg.nii.gz\")\n",
    "        if n < 0.8*data_num:  # Train\n",
    "            nii_to_jpg(view_pla_path, os.path.join(\"Train\", \"Image\", file_name + \"_pla\"))\n",
    "            nii_to_jpg(view_seg_path, os.path.join(\"Train\", \"Target\", file_name + \"_seg\"))\n",
    "        else: # Val\n",
    "            nii_to_jpg(view_pla_path, os.path.join(\"Val\", \"Image\", file_name + \"_pla\"))\n",
    "            nii_to_jpg(view_seg_path, os.path.join(\"Val\", \"Target\", file_name + \"_seg\"))\n",
    "\n",
    "print(\"Finish Segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (Optional)\n",
    "\n",
    "Images in the original dataset are usually in different sizes, so sometimes we need to resize and normalise (z-score is commonly used in preprocessing the MRI images) them to fit the CNN model. Depending on the images you choose to use for training your model, some other preprocessing methods. If preprocessing methods like cropping is applied, remember to convert the segmentation result back to its original size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580a34bd6483441082a18f9d261c151e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='val', max=154), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# class DataPreProcessor:\n",
    "#     def __init__(self, list_IDs, batch_size=32, dim=(240,240), n_channels=3,\n",
    "#                  n_classes=2, shuffle=True):\n",
    "#         'Initialization'\n",
    "#         self.dim = dim\n",
    "#         self.batch_size = batch_size\n",
    "#         self.list_IDs = list_IDs\n",
    "#         self.n_channels = n_channels\n",
    "#         self.n_classes = n_classes\n",
    "#         self.shuffle = shuffle\n",
    "#         self.on_epoch_end()\n",
    "\n",
    "\n",
    "#     def DataExtract(self, data_path):\n",
    "#         dirs = os.listdir(data_path)\n",
    "#         dirs.sort()\n",
    "#         for CLASS in dirs:\n",
    "#             if not CLASS.startswith(\".\"):\n",
    "#                 vimg_pla_path = os.path.join(data_path, \"train\", str(CLASS), str(CLASS) + \"_pla.nii.gz\")\n",
    "#                 vimg_seg_path = os.path.join(data_path, \"train\", str(CLASS), str(CLASS) + \"_seg.nii.gz\")\n",
    "#                 vimg_pla_load = nib.load(vimg_pla_path).get_fdata()\n",
    "#                 vimg_seg_load = nib.load(vimg_seg_path).get_fdata()\n",
    "#                 for i in vimg_pla_load.shape[2]:\n",
    "#                     img_pla = vimg_pla_load[:,:,i]\n",
    "#                     img_seg = vimg_seg_load[:,:,i]\n",
    "\n",
    "#     # def ImgProcess(self, data_path):\n",
    "        \n",
    "                    \n",
    "                    \n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-time data augmentation\n",
    "Generalizability is crucial to a deep learning model and it refers to the performance difference of a model when evaluated on the seen data (training data) versus the unseen data (testing data). Improving the generalizability of these models has always been a difficult challenge. \n",
    "\n",
    "**Data Augmentation** is an effective way of improving the generalizability, because the augmented data will represent a more comprehensive set of possible data samples and minimizing the distance between the training and validation/testing sets.\n",
    "\n",
    "There are many data augmentation methods you can choose in this projects including rotation, shifting, flipping, etc.\n",
    "\n",
    "You are encouraged to try different augmentation method to get the best segmentation result.\n",
    "\n",
    "\n",
    "## Get the data generator ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataGenerator(tensorflow.keras.utils.Sequence):\n",
    "#     'Generates data for Keras'\n",
    "#     def __init__(self, list_IDs, batch_size=32, dim=(240,240), n_channels=3,\n",
    "#                  n_classes=2, shuffle=True):\n",
    "#         'Initialization'\n",
    "#         self.dim = dim\n",
    "#         self.batch_size = batch_size\n",
    "#         self.list_IDs = list_IDs\n",
    "#         self.n_channels = n_channels\n",
    "#         self.n_classes = n_classes\n",
    "#         self.shuffle = shuffle\n",
    "#         self.on_epoch_end()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         'Denotes the number of batches per epoch'\n",
    "#         return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         'Generate one batch of data'\n",
    "#         # Generate indexes of the batch\n",
    "#         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "#         # Find list of IDs\n",
    "#         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "#         # Generate data\n",
    "#         X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "#         return X, y\n",
    "\n",
    "#     def on_epoch_end(self):\n",
    "#         'Updates indexes after each epoch'\n",
    "#         self.indexes = np.arange(len(self.list_IDs))\n",
    "#         if self.shuffle == True:\n",
    "#             np.random.shuffle(self.indexes)\n",
    "\n",
    "#     def __data_generation(self, list_IDs_temp):\n",
    "#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         # Initialization\n",
    "#         X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "#         y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "#         # Generate data\n",
    "#         for i, ID in enumerate(list_IDs_temp): # there requires fix\n",
    "#             # Store sample\n",
    "#             # Add data augmentation here\n",
    "#             X[i,] = np.load(ID)\n",
    "\n",
    "#             # Store class\n",
    "#             y[i] = min(1,np.sum(np.load(ID.split('_')[0]+'_seg.npy')))\n",
    "\n",
    "#         return X, tensorflow.keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    batch_size,\n",
    "    img_size,\n",
    "    input_img_paths,\n",
    "    target_img_paths,\n",
    "    max_dataset_len=None,\n",
    "):\n",
    "    \"\"\"Returns a TF Dataset.\"\"\"\n",
    "\n",
    "    def load_img_masks(input_img_path, target_img_path):\n",
    "        input_img = tf_io.read_file(input_img_path)\n",
    "        input_img = tf_io.decode_png(input_img, channels=3)\n",
    "        input_img = tf_image.resize(input_img, img_size)\n",
    "        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
    "\n",
    "        target_img = tf_io.read_file(target_img_path)\n",
    "        target_img = tf_io.decode_png(target_img, channels=1)\n",
    "        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
    "        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n",
    "\n",
    "        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "        target_img -= 1\n",
    "        return input_img, target_img\n",
    "\n",
    "    # For faster debugging, limit the size of data\n",
    "    if max_dataset_len:\n",
    "        input_img_paths = input_img_paths[:max_dataset_len]\n",
    "        target_img_paths = target_img_paths[:max_dataset_len]\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
    "    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_dir = \"Train/Image/\"\n",
    "train_target_dir = \"Train/Target/\"\n",
    "val_input_dir = \"Val/Image/\"\n",
    "val_target_dir = \"Val/Target/\"\n",
    "img_size = (250, 250)\n",
    "num_classes = 3\n",
    "batch_size = 4\n",
    "\n",
    "train_input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(train_input_dir, fname)\n",
    "        for fname in os.listdir(train_input_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(val_input_dir, fname)\n",
    "        for fname in os.listdir(val_input_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(train_target_dir, fname)\n",
    "        for fname in os.listdir(train_target_dir)\n",
    "        if fname.endswith(\".jpg\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(val_target_dir, fname)\n",
    "        for fname in os.listdir(val_target_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of Train samples:\", len(train_input_img_paths))\n",
    "\n",
    "# for input_path, target_path in zip(train_input_img_paths[:10], train_target_img_paths[:10]):\n",
    "#     print(input_path, \"|\", target_path)\n",
    "\n",
    "print(\"Number of Val samples:\", len(val_input_img_paths))\n",
    "\n",
    "# for input_path, target_path in zip(val_input_img_paths[:10], val_target_img_paths[:10]):\n",
    "#     print(input_path, \"|\", target_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_img_paths_ = train_input_img_paths[:]\n",
    "train_target_img_paths_ = train_target_img_paths[:]\n",
    "\n",
    "val_input_img_paths_ = val_input_img_paths[:]\n",
    "val_target_img_paths_ = val_target_img_paths[:]\n",
    "\n",
    "train_dataset = get_dataset(\n",
    "    batch_size,\n",
    "    img_size,\n",
    "    train_input_img_paths,\n",
    "    train_target_img_paths,\n",
    "    max_dataset_len=1000,\n",
    ")\n",
    "\n",
    "valid_dataset = get_dataset(\n",
    "    batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a metric for the performance of the model\n",
    "Dice score is used here to evaluate the performance of your model.\n",
    "More details about the Dice score and other metrics can be found at \n",
    "https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2. Dice score can be also used as the loss function for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own model here\n",
    "The U-Net (https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28) structure is widely used for the medical image segmentation task. You can build your own model or modify the UNet by changing the hyperparameters for our task. If you choose to use Keras, more information about the Keras layers including Conv2D, MaxPooling and Dropout can be found at https://keras.io/api/layers/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = get_model(img_size, num_classes)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model here\n",
    "Once you defined the model and data generator, you can start training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-4), loss=\"sparse_categorical_crossentropy\"\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 50\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "Once your model is trained, remember to save it for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model on the test set\n",
    "After your last Q&A session, you will be given the test set. Run your model on the test set to get the segmentation results and submit your results in a .zip file. If the MRI image is named '100_fla.nii.gz', save your segmentation result as '100_seg.nii.gz'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
